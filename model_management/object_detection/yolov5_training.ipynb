{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Goal**</u>:\n",
    "Learn to load the yolo v5 model using Dataloop platform\n",
    "run a train flow for the model\n",
    "\n",
    "### <u>**Background**</u>:\n",
    "Yolo V5 model is an extension to the Yolo Family. (V5 was created by 'ultralytics')\n",
    "\n",
    "the V5 key new added features are part of the train function: using mosaic augmetation and finetuning the anchors\n",
    "the model is implemented in PyTorch framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib as mpl\n",
    "# mpl.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import dtlpy as dl\n",
    "from dtlpy import ml \n",
    "from dtlpy.ml import train_utils\n",
    "# from dtlpy.ml import train_utils\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "\n",
    "first we will get the dataloop entities\n",
    "\n",
    "Then we can create our Model Adapter `adapter` using the `model.build()` method.\n",
    "\n",
    "`adapter` is an instace of the model set to work with our datloop platform.\n",
    "this contains method like `train()` and `predict_items()` and more usefull methods for building your model using our platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dl.models.get('yolo-v5', model_id=None)\n",
    "model.to_df()\n",
    "# verbose and model.snapshots.list().to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = model.snapshots.get(snapshot_name='pretrained-yolo-v5')\n",
    "verbose and snapshot.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciate the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapter = model.build()\n",
    "# local develop ==>\n",
    "adapter = model.build()\n",
    "\n",
    "# override cpu\n",
    "adapter._set_device('cpu')\n",
    "adapter.logger.handlers[0].setLevel('DEBUG')\n",
    "\n",
    "adapter.load_from_snapshot(snapshot=snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Example\n",
    "\n",
    "we will use a single item to show how to run inference easily\n",
    "\n",
    "USER: please choose an item on your own dataset - so you have premission to view and edit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dl.items.get(item_id='611e174e4c09acc3c5bb81d3')\n",
    "image = np.asarray(Image.open(item.download()))\n",
    "\n",
    "# use the adapter to inference without uploading annotations\n",
    "all_annotations = adapter.predict_items([item], with_upload=False, min_score=0.1)\n",
    "\n",
    "item_annotations = all_annotations[0]  # predict items returns a list for each of the input items\n",
    "print(item_annotations.print(to_return=True))\n",
    "# print(item.annotations.list().print(to_return=True))\n",
    "print(item_annotations.to_df())\n",
    "\n",
    "# add the items to the image\n",
    "annotated_image = item.annotations.show(image=image, thickness=5)\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Training\n",
    "\n",
    "The \"prepare_dataset()\" method prepares the raw dataset to be a part of a snapshot and ready for training.  \n",
    "This will:\n",
    "1. Clone the dataset to prepare \n",
    "2. Create train, validation and test splits\n",
    "3. Lock the dataset to be read only\n",
    "\n",
    "We can set the partition split to be random (using float to set the precentage) or we can use DQL filters to set specific items and folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dl.projects.create('Fruits Proj')\n",
    "dataset = project.datasets.create('FruitImage')\n",
    "# # split the dataset to 2 partitions - 80% train 20% validation - randomly\n",
    "# partitions = {dl.SnapshotPartitionType.TRAIN: 0.8,\n",
    "#               dl.SnapshotPartitionType.VALIDATION: 0.2}\n",
    "\n",
    "# use DQL to set the two directories and the train/val split\n",
    "partitions = {dl.SnapshotPartitionType.TRAIN: dl.Filters(field='dir', values='/train'),\n",
    "              dl.SnapshotPartitionType.VALIDATION: dl.Filters(field='dir', values='/test')}\n",
    "\n",
    "cloned_dataset = train_utils.prepare_dataset(dataset,\n",
    "                                             filters=None,\n",
    "                                             partitions=partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Snapshot\n",
    "When running the inferene we used the pretrained `Snapshot`.\n",
    "    This has the weights of the model saved in a GCS bucket for our global model\n",
    "\n",
    "Now, when runnig our own specific detector we will create a new `Snapshot`\n",
    "    * Item bucket - saves the weights to a dedicated item in the dataloop platform\n",
    "    * Set to specific labels - what are the classes the model will work on\n",
    "    * Update configuration and other tags etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_name='fruit-first'\n",
    "try:\n",
    "    new_snapshot = model.snapshots.get(snapshot_name=snapshot_name)\n",
    "except Exception:\n",
    "    bucket = project.buckets.create(bucket_type=dl.BucketType.ITEM, model_name=model.name, snapshot_name=snapshot_name)\n",
    "    new_snapshot = snapshot.clone(snapshot_name=snapshot_name,\n",
    "                                dataset_id=cloned_dataset.id,\n",
    "                                bucket=bucket,\n",
    "                                configuration={'batch_size': 2,\n",
    "                                                'start_epoch': 0,\n",
    "                                                'max_epoch': 5,\n",
    "                                                }\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cloned snapshot\n",
    "Note that the new snapshot was cloned with all it's content.\n",
    "\n",
    "This means that the Item Bucket was also copied and contains the same stat as the 'pretrained'\n",
    "\n",
    "This will be true until we will train the model and save out new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_snapshot.bucket.list_content().to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "We are almost ready to train\n",
    "\n",
    "We will use the adapter methods to run the train:\n",
    "\n",
    "    1. Load the snapshot to the adapter - contains the dataset to train on, and all the configuraions\n",
    "    2. Download the dataset locally - saves it to disk\n",
    "        * use `adapter.convert_from_dtlpy()`  if need to convert the dataloop format\n",
    "    3. run the train!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter.load_from_snapshot(snapshot=new_snapshot)\n",
    "root_path, data_path, output_path = adapter.prepare_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training {!r} with snapshot {!r} on data {!r}\".format(model.name, new_snapshot.id, data_path))\n",
    "adapter.train(data_path=data_path, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter.save_to_snapshot(local_path=output_path, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict New Snapshot on Local Item\n",
    "\n",
    "We will run a the simple predict.\n",
    "only this time we will load a different sanpshot to the adapter - the one we just trained and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter.load_from_snapshot(snapshot=new_snapshot)\n",
    "\n",
    "item = dl.items.get(item_id='6110d4a41467ded7a8c2a23d')\n",
    "image = Image.open(item.download())\n",
    "\n",
    "all_annotations = adapter.predict_items([item], with_upload=True)  # use with_upload = False\n",
    "plt.imshow(item.annotations.show(np.asarray(image), thickness=5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "889e37e1a2a0216c0e113c3ef8ca898b88cd7e4916c9f79f9b4146c1fe0dbcb6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
