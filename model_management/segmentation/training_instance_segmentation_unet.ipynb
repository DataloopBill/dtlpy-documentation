{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train UNet Model\n",
    "\n",
    "In this Notebook you will learn how to train your UNet architecture with Dataloop and Pytorch\n",
    "\n",
    "UNet is an Encoder - Decoder architecture for creating segmentation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dtlpy as dl\n",
    "from dtlpy.ml import train_utils\n",
    "from dtlpy.ml.dataset_generators.torch_dataset_generator import DataGenerator\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the GPU\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the DataLoop entities\n",
    "\n",
    "lets get the model and dataset entities from our dataloop platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dl.models.get('unet')  # This is the global model\n",
    "# Data entities\n",
    "project = dl.projects.get('shefi-contests', '50f0fc03-4d70-455d-b485-c78cca53f2be')\n",
    "dataset = dl.datasets.get('carvana', '61b9bbc1e8ad454a9aa7d285')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot\n",
    "\n",
    "Now we can create a new snapshot - we will add your name and data to the suffix to make the snapshot has a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoami = dl.client_api.info()['user_email']\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Create a new snapshot - personally and with currect datetime\n",
    "snapshot_name = f\"carvana-train-example-{whoami.split('@')[0]}-{now.isoformat(timespec='minutes')}\"\n",
    "snapshot = model.snapshots.create(\n",
    "    snapshot_name=snapshot_name,\n",
    "    dataset_id=dataset.id,\n",
    "    description='train unet example',\n",
    "    bucket=project.buckets.create(bucket_type=dl.BucketType.ITEM, model_name=model.name, snapshot_name=snapshot_name),\n",
    "    tags=['example', 'notebook'],\n",
    "    configuration={'id_to_label_map': {'1': 'car'},\n",
    "                   'image_normalize_mu': 0, 'image_normalize_std': 1,\n",
    "                   'input_shape': [640, 960], 'batch_size': 2,\n",
    "                   'num_epochs': 2},\n",
    "    project_id=project.id,\n",
    "    labels=['car']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets View the Model and Snapshot entities\n",
    "\n",
    "We use the to_df in order to convert to a DataFrame and view it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One last thing to make sure before we train\n",
    "\n",
    "Our `adapter` train method expects the data to be organized as: train-validation-test  \n",
    "this can be created manually on small datasets using `train_utils.create_dataset_partition()`\n",
    "\n",
    "Our dataset is already prepared, we will just verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_items = dataset.get_partitions(partitions=dl.SnapshotPartitionType.TRAIN)\n",
    "val_items = dataset.get_partitions(partitions=dl.SnapshotPartitionType.VALIDATION)\n",
    "test_items = dataset.get_partitions(partitions=dl.SnapshotPartitionType.TEST)\n",
    "\n",
    "print(f\"Dataset {dataset.name} Data partition, TRAIN: {train_items.items_count}, VALIDATION {val_items.items_count}, TEST {test_items.items_count} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we can start to train\n",
    "\n",
    "We initialize the adapter using the `build` method.\n",
    "\n",
    "The `Adapter` is the base class to connect between dataloop platform and our specific model  \n",
    "some method are inheritance from the base adapter and some are written specifically per model\n",
    "each architecture has it's own adapter which you can view it's raw code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = model.build()\n",
    "adapter.load_from_snapshot(snapshot=snapshot)\n",
    "# adapter._set_adapter_handler('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path, data_path, output_path = adapter.prepare_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter.train(data_path=data_path, output_path=output_path,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SAVING\n",
    "\n",
    "The current adapter now holds the best model fit for our data.\n",
    "\n",
    "In order to upload the weights and other configurations we need to save our snapshot.  \n",
    "We will use a temp dir - so we save all content to that dir and upload it (other option is to upload all the *`output_path`* which has more runtime files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = tempfile.mkdtemp(prefix=snapshot.name, suffix=now.strftime('%F-%H%M%S'))\n",
    "adapter.save_to_snapshot(local_path=temp_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING THE MODEL - PREDICTION\n",
    "\n",
    "We will use the DataGenerator to view the image (this utility already connects with our dataloop item and annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = DataGenerator(data_path=os.path.join(data_path, 'validation'),\n",
    "                        dataset_entity=snapshot.dataset,\n",
    "                        annotation_type=dl.AnnotationType.SEGMENTATION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example - get 1 entry and visualize it\n",
    "datagen.visualize(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Item\n",
    "\n",
    "Our data generator returns Data Item dictionary  \n",
    "we can parse it to get the item and annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item = datagen[20]\n",
    "print(f\"To get the item_id from the dataItem ({data_item.keys()}) object we can use the ann json\")\n",
    "ann_json = json.load(open(data_item['annotation_filepath'], 'r'))\n",
    "item_id = ann_json['id']\n",
    "item = dl.items.get(item_id=item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = adapter.predict_items(items=[item], with_upload=False)\n",
    "item_predictions = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can ignore the label 0 which usually uses for background\n",
    "predictions = adapter.predict_items(items=[item], with_upload=False, with_bg=False)\n",
    "item_sematic_preds = predictions[0]\n",
    "item_sematic_preds.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create polygons instead of sematic segmetations\n",
    "predictions = adapter.predict_items(items=[item], with_upload=False, to_poly=True)\n",
    "item_polygons_preds = predictions[0]\n",
    "item_polygons_preds.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = item_polygons_preds.show(data_item['image'], thickness=5 )\n",
    "plt.imshow(annotated)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e234d837eebba94c7397b4e38c0b82bd3e4741cb2c390182a3fb441eaf8f3cd5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
