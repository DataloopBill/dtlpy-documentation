{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Create an Azure blob function to Continuously Sync a Bucket with Dataloop's Dataset  \n", "  \n", "If you want to catch events from the Azure blob and update the Dataloop Dataset you need to set up a blob function.  \n", "The function will catch the blob storage events and will reflect them into the Dataloop Platform.  \n", "  \n", "### Create the blob function  \n", "1. Insert to Azure platform  \n", "2. Create a Resource group  \n", "   * Choose Subscription, Name and Region  \n", "3. Insert to the Resource group  \n", "4. Create -> Storage account  \n", "   * Choose Subscription, your Resource group, Name and Region  \n", "5. Create a Container in the created Storage account  \n", "   * Public access level -> Container OR Blob  \n", "6. Go back to Resource group  \n", "7. Create -> Function App  \n", "   * Choose Subscription, your Resource group, Name and Region  \n", "   * Publish -> Code  \n", "   * Runtime stack -> Python  \n", "   * Version -> <=3.7  \n", "8. Need to flow the instructions in [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python) to configure your environment and deploy the function according to this order:  \n", "   1. Configure your environment  \n", "   2. Sign in to Azure  \n", "   3. Create your local project  \n", "      * in Select a template for your project's first function choose -> Azure Blob Storage trigger  \n", "      * in Storage account select your Storage account that created in step4  \n", "      * in Resource group select your Resource group that created in step1  \n", "      * open the code file  \n", "      * add dtlpy to the requirements.txt file  \n", "      * add \"disabled\": false to the function.json file  \n", "      * add a function code to __init__.py file  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import azure.functions as func\n", "import dtlpy as dl\n", "import os\n", "", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "", "", "def main(myblob: func.InputStream):\n", "    dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "    dataset = dl.datasets.get(dataset_id=dataset_id,\n", "                              fetch=False  # to avoid GET the dataset each time\n", "                              )\n", "", "    # remove th Container name from the path\n", "    path_parser = myblob.name.split('/')\n", "    file_name = '/'.join(path_parser[1:])\n", "", "    file_name = 'external://' + file_name\n", "    dataset.items.upload(local_path=file_name)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["   4. Deploy the project to Azure to the function app that you create in step6  \n", "   5. Set the Crate a Azure Blob Storage trigger to your Container Name  \n", "   ![add_layer](../../../../assets/azure-blob/trriggerDataset.png)  \n", "   6. In VS code go to view tab -> Command Palette -> Azure Functions: Upload Local Settings  \n", "9. go to the Function App -> insert to your function -> Function -> Function App  \n", "       * add the 3 secrets vars DATASET_ID, DTLPY_USERNAME, DTLPY_PASSWORD  \n", "  \n", "### now all what you add to your Container will add auto to dataloop dataset  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}