{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Dataset Binding with Azure  \n", "  \n", "We will create an Azure Function App to continuously sync a blob with Dataloop's dataset  \n", "  \n", "If you want to catch events from the Azure blob and update the Dataloop Dataset you need to set up a blob function.  \n", "The function will catch the blob storage events and will reflect them into the Dataloop Platform.  \n", "  \n", "If you are familiar with [Azure Function App](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python), you can just use our integration function below.  \n", "  \n", "We assume you already have an Azure account with resource group and storage account. If you don't, follow the [Azure docs](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create) and create them.  \n", "  \n", "### Create the Blob Function  \n", "1. Create a Container in the created Storage account  \n", "   * Public access level -> Container OR Blob  \n", "NOTE: This container should be used as the external storage for the Dataloop dataset.  \n", "2. Go back to Function App and click create -> to create a new function  \n", "   * Choose Subscription  \n", "   * Choose your Resource group  \n", "   * Choose Function name  \n", "   * Publish -> Code  \n", "   * Runtime stack -> Python  \n", "   * Version -> 3.10 >= Version >= 3.7  \n", "   NOTE: when choosing python 3.7 please pay attention to the AOL warning  \n", "   * Choose Region  \n", "   * Use default values for all other options (OS and Plan ...)  \n", "   * Press next and choose your Storage account  \n", "   * Review and create  \n", "  \n", "### Deploy your function  \n", "In VS code, flow the instructions in [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python) to configure your environment and deploy the function:  \n", "1. Configure your environment  \n", "2. Sign in to Azure  \n", "3. Create your local project  \n", "   * Choose the directory location for your project workspace and choose Select.  \n", "    You should either create a new folder or choose an empty folder for the project workspace.  \n", "    Don't choose a project folder that is already part of a workspace.  \n", "   * In Select a template for your project's first function choose -> Azure Event Grid trigger  \n", "   * Open the code file  \n", "   * In the requirements.txt file -> add ```dtlpy```  \n", "   * Replace the code on \\_\\_init\\_\\_.py file with the presented code snippet  \n", "   NOTE: Make sure you **save** the file on Vs code - If not it **will not be deployed** to Azure  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import azure.functions as func\n", "import dtlpy as dl\n", "import os\n", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "container_name = os.environ.get('CONTAINER_NAME')\n", "", "def main(event: func.EventGridEvent):\n", "    url = event.get_json()['url']\n", "    if container_name in url:\n", "        dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "        dataset = dl.datasets.get(dataset_id=dataset_id,\n", "                                  fetch=False  # to avoid GET the dataset each time\n", "                                  )\n", "        # remove th Container name from the path\n", "        file_name = url.split(container_name)[1]\n", "        if 'BlobCreated' in event.event_type:\n", "            file_name = 'external:/' + file_name\n", "            dataset.items.upload(local_path=file_name)\n", "        else:\n", "            dataset.items.delete(filename=file_name)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Deploy the code to the function app you created - For more info take a look at [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python?pivots=python-mode-configuration#deploy-the-project-to-azure)  \n", "5. In VS code go to view tab -> Command Palette -> Azure Functions: Upload Local Settings  \n", "6. Go to the Function App -> Select your function -> Configuration (Under Settings section)  \n", "       * Add the 4 secrets vars `DATASET_ID`, `DTLPY_USERNAME`, `DTLPY_PASSWORD`, `CONTAINER_NAME` (your container that want to trigger it)  \n", "            * To populate the values for the vars: `DTLPY_USERNAME`, `DTLPY_PASSWORD` you have 2 options:  \n", "                * Option 1: Use your **registered** dataloop user and password  \n", "                * Option 2: Create a **DataLoop Bot** on your project using this code  \n", "  \n", "  \n", "If you choose option 2 follow these steps:  \n", "    1. Open your favorite IDE (Doesn't matter where)  \n", "    2. Open a new file  \n", "    3. Copy this code snippet and run it  \n", "    NOTE: Make sure the project name is the same project where the desired upstream dataset is located  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "project = dl.projects.get(project_name='project name')\n", "bot = project.bots.create(name='bot name', return_credentials=True)\n", "print('username', bot.id)\n", "print('password', bot.password)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["7. Go to Function App -> Select your function -> Navigate in the sidebar to the functions tab and select your function ->  \n", "Integration -> Select the trigger -> Create Event Grid subscription  \n", "    * Event Schema -> Event Grid Schema  \n", "    * Topic Types -> Storage Account (Blob & GPv2)  \n", "    * Select your Subscription, Resource Group, Resource  \n", "    * System Topic Name -> your Event Grid Topic (if you do not have one create it)  \n", "    * Filter to Event Types -> Create and Delete  \n", "    * Endpoint Type -> Function App (Azure function)  \n", "    * Endpoint -> your function  \n", "  \n", "**NOTE:** It will take up to 5 mins when you deploy using auto upstream  \n", "  \n", "  \n", "**Done! Now your storage blob will be synced with the Dataloop dataset**  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}