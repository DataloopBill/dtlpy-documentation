{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Cloud Storage  \n", "## External Storage Dataset  \n", "  \n", "If you already have your data managed and organized on a cloud storage service, such as GCS/S3/Azure, you may want to  \n", "utilize that with Dataloop, and not upload the binaries and create duplicates.  \n", "  \n", "### Cloud Storage Integration  \n", "  \n", "Access & Permissions - Creating an integration with GCS/S2/Azure cloud requires adding a key/secret with the following  \n", "permissions:  \n", "  \n", "List (Mandatory) - allowing Dataloop to list all of the items in the storage.  \n", "Get (Mandatory) - get the items and perform pre-process functionalities like thumbnails, item info etc.  \n", "Put / Write (Mandatory) - lets you upload your items  \n", "directly to the external storage from the Dataloop platform.  \n", "Delete - lets you delete your items directly from the external storage using the Dataloop platform.  \n", "  \n", "### Create Integration With GCS  \n", "  \n", "### Creating an integration GCS requires having JSON file with GCS configuration.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name=org_name)\n", "with open(r\"C:\\gcsfile.json\", 'r') as f:\n", "    gcs_json = json.load(f)\n", "gcs_to_string = json.dumps(gcs_json)\n", "organization.integrations.create(name='gcsintegration',\n", "                                 integrations_type=dl.ExternalStorage.GCS,\n", "                                 options={'key': '',\n", "                                          'secret': '',\n", "                                          'content': gcs_to_string})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create Integration With S3  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name='my-org')\n", "organization.integrations.create(name='S3integration', integrations_type=dl.ExternalStorage.S3,\n", "                                 options={'key': \"my_key\", 'secret': \"my_secret\"})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create Integration With Azure  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name='my-org')\n", "organization.integrations.create(name='azureintegration',\n", "                                 integrations_type=dl.ExternalStorage.AZUREBLOB,\n", "                                 options={'key': 'my_key',\n", "                                          'secret': 'my_secret',\n", "                                          'clientId': 'my_clientId',\n", "                                          'tenantId': 'my_tenantId'})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Storage Driver  \n", "  \n", "Once you have an integration, you can set up a driver, which adds a specific bucket (and optionally with a specific  \n", "path/folder) as a storage resource.  \n", "  \n", "### Create Drivers in the Platform (browser)  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# param name: the driver name\n", "# param driver_type: ExternalStorage.S3, ExternalStorage.GCS , ExternalStorage.AZUREBLOB\n", "# param integration_id: the integration id\n", "# param bucket_name: the external bucket name\n", "# param project_id:\n", "# param allow_external_delete:\n", "# param region: relevant only for s3 - the bucket region\n", "# param storage_class: relevant only for s3\n", "# param path: Optional. By default, path is the root folder. Path is case sensitive.\n", "# return: driver object\n", "import dtlpy as dl\n", "project = dl.projects.get('prject_name')\n", "driver = project.drivers.create(name='driver_name',\n", "                                driver_type=dl.ExternalStorage.S3,\n", "                                integration_id='integration_id',\n", "                                bucket_name='bucket_name',\n", "                                allow_external_delete=True,\n", "                                region='eu-west-1',\n", "                                storage_class=\"\",\n", "                                path=\"\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once the integration and drivers are ready, you can create a Dataloop Datsaset and sync all the data:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# create a dataset from a driver name, you can also create by the driver ID\n", "import dtlpy as dl\n", "project: dl.Project\n", "dataset = project.datasets.create(dataset_name=dataset_name,\n", "                                  driver=driver)\n", "", "dataset.sync()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create an AWS Lambda to Continuously Sync a Bucket with Dataloop's Dataset  \n", "  \n", "If you want to catch events from the AWS bucket and update the Dataloop Dataset you need to set up a Lambda.  \n", "The Lambda will catch the AWS bucket events and will reflect them into the Dataloop Platform.  \n", "  \n", "We have prepared an environment zip file with our SDK for python3.8 so you don't need to create anything else to use dtlpy in the lambda.  \n", "  \n", "NOTE: For any other custom use (e.g other python version or more packages) try creating your own layer (We used [this](https://www.geeksforgeeks.org/how-to-install-python-packages-for-aws-lambda-layers) tutorial and the python:3.8 docker image).  \n", "  \n", "### Create the Lambda  \n", "1. Create a new Lambda  \n", "2. The default timeout is 3[s] so we'll need to change to 1[m]:  \n", "    Configuration \u2192 General configuration \u2192 Edit \u2192 Timeout  \n", "3. Copy the following code:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import os\n", "import urllib.parse\n", "", "# Set dataloop path to tmp (to read/write from the lambda)\n", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "import dtlpy as dl\n", "", "DATASET_ID = ''\n", "DTLPY_USERNAME = ''\n", "DTLPY_PASSWORD = ''\n", "", "def lambda_handler(event, context):\n", "    dl.login_m2m(email=DTLPY_USERNAME, password=DTLPY_PASSWORD)\n", "    dataset = dl.datasets.get(dataset_id=DATASET_ID,\n", "                              fetch=False  # to avoid GET the dataset each time\n", "                              )\n", "", "    for record in event['Records']:\n", "        # Get the bucket name\n", "        bucket = record['s3']['bucket']['name']\n", "", "        # Get the file name\n", "        filename = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')\n", "", "        if 'ObjectRemoved' in record['eventName']:\n", "            # On delete event - delete the item from Dataloop\n", "            try:\n", "                dtlpy_filename = '/' + filename\n", "                filters = dl.Filters(field='filename', values=dtlpy_filename)\n", "                dataset.items.delete(filters=filters)\n", "            except Exception as e:\n", "                raise e\n", "", "        elif 'ObjectCreated' in record['eventName']:\n", "            # On create event - add a new item to the Dataset\n", "            try:\n", "                # upload the file\n", "                path = 'external://' + filename\n", "                # dataset.items.upload(local_path=path, overwrite=True) # if overwrite is required\n", "                dataset.items.upload(local_path=path)\n", "            except Exception as e:\n", "                raise e\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Add a Layer to the Lambda  \n", "We have created an AWS Layer with the Dataloop SDK ready. Click [here](https://storage.googleapis.com/dtlpy/aws-python3.8-lambda-layer/layer.zip) to download the zip file.  \n", "Because the layer's size is larger than 50MB you cannot use it directly (AWS restrictions), but need to upload it to a bucket first.  \n", "Once uploaded, create a new layer for the dtlpy env:  \n", "1. Go to the layers screen and \"click Add Layer\".  \n", "![add_layer](../../../assets/aws-lambda-screenshots/create_layer.png)  \n", "2. Choose a name (dtlpy-env).  \n", "3. Use the link to the bucket layer.zip.  \n", "4. Select the env (x86_64, python3.8).  \n", "5. Click \"Create\" and the bottom on the page.  \n", "  \n", "Go back to your lambda and add the layer:  \n", "1. Select the \"Add Layer\".  \n", "![add_layer](../../../assets/aws-lambda-screenshots/add_layer.png)  \n", "2. Choose \"Custom layer\" and select the Layer you've added and the version.  \n", "3. click \"Add\" at the bottom.  \n", "  \n", "### Create the Bucket Events  \n", "Go to the bucket you are using, and create the event:  \n", "1. Go to Properties \u2192 Event notifications \u2192 Create event notification  \n", "1. Choose a name for the Event  \n", "1. For Event types choose: All object create events, All object delete events  \n", "1. Destination - Lambda function \u2192 Choose from your Lambda functions \u2192 choose the function you build \u2192 SAVE  \n", "  \n", "Deploy and you're good to go!  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}