{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Step-by-step: Setting up auto-annotation model  \n", "This tutorial explains step-by-step how to upload your model to the Dataloop platform and use it for auto annotate any item  \n", "  \n", "### Your Model  \n", "Following this example you can use your own model, or the sample one presented below.  \n", "It uses CV2 and Caffe for basic face detection.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import os\n", "import cv2\n", "import dtlpy as dl\n", "class ServiceRunner:\n", "    def __init__(self,\n", "                 model_filename: str,\n", "                 prototxt_filename: str,\n", "                 min_confidence: float):\n", "        prototxt = os.path.join(os.getcwd(), prototxt_filename)\n", "        weights = os.path.join(os.getcwd(), model_filename)\n", "        print(\"[INFO] loading model...\")\n", "        self.net = cv2.dnn.readNetFromCaffe(prototxt, weights)\n", "        self.min_confidence = min_confidence\n", "", "    def detect(self, item: dl.Item):\n", "        print(\"[INFO] downloading image...\")\n", "        filename = item.download()\n", "        try:\n", "            # load the input image and construct an input blob for the image\n", "            # by resizing to a fixed 300x300 pixels and then normalizing it\n", "            print(\"[INFO] opening image...\")\n", "            image = cv2.imread(filename)\n", "            (h, w) = image.shape[:2]\n", "            blob = cv2.dnn.blobFromImage(cv2.resize(image,\n", "                                                    (300, 300)), 1.0,\n", "                                         (300, 300),\n", "                                         (104.0, 177.0, 123.0))\n", "            # pass the blob through the network and obtain the detections and\n", "            # predictions\n", "            print(\"[INFO] computing object detections...\")\n", "            self.net.setInput(blob)\n", "            detections = self.net.forward()\n", "            # create annotation builder to add annotations to item\n", "            print(\"[INFO] uploading annotations...\")\n", "            builder = item.annotations.builder()\n", "            # loop over the detections\n", "            for i in range(0, detections.shape[2]):\n", "                # extract the confidence (i.e., probability) associated with the\n", "                # prediction\n", "                confidence = detections[0, 0, i, 2]\n", "                # filter out weak detections by ensuring the `confidence` is\n", "                # greater than the minimum confidence\n", "                if confidence > self.min_confidence:\n", "                    # compute the (x, y)-coordinates of the bounding box for the\n", "                    # object\n", "                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n", "                    (startX, startY, endX, endY) = box.astype(\"int\")\n", "                    # draw the bounding box of the face along with the associated\n", "                    # probability\n", "                    builder.add(\n", "                        annotation_definition=dl.Box(\n", "                            top=startY,\n", "                            left=startX,\n", "                            right=endX,\n", "                            bottom=endY,\n", "                            label='person'\n", "                        ),\n", "                        model_info={\n", "                            'name': 'Caffe',\n", "                            'confidence': confidence\n", "                        }\n", "                    )\n", "                    # upload annotations\n", "            builder.upload()\n", "            print(\"[INFO] Done!\")\n", "        finally:\n", "            os.remove(filename)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Define the module  \n", "In this example, we load the model in the init method.  \n", "This method runs only once at deployment time. This can save us time by avoiding to load the model at each execution.  \n", "The init inputs are attributes that we want the service to include for its entire lifetime.  \n", "In this case, it's the model and weights files we want the service to use and the confidence limit of which to accept detections.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["module = dl.PackageModule(\n", "    init_inputs=[\n", "        dl.FunctionIO(name='model_filename', type=dl.PackageInputType.JSON),\n", "        dl.FunctionIO(name='prototxt_filename', type=dl.PackageInputType.JSON),\n", "        dl.FunctionIO(name='min_confidence', type=dl.PackageInputType.JSON)\n", "    ],\n", "    functions=[\n", "        dl.PackageFunction(\n", "            name='detect',\n", "            description='OpenCV face detection using Caffe model',\n", "            inputs=[\n", "                dl.FunctionIO(name='item', type=dl.PackageInputType.ITEM)\n", "            ]\n", "        )\n", "    ]\n", ")\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Weights File  \n", "  \n", "The function uses 2 files that hold the model and weight to run the detection.  \n", "We need to have these files at the same folder as the entry point.  \n", "To get these files please download them here.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Package Requirements  \n", "  \n", "Our package's codebase uses 2 Python libraries that are not standard libraries.  \n", "Therefore, we need to make sure they are pre-installed before running the entry point.  \n", "One way to do so is to use a custom Docker Image (information on this process can be found here.  \n", "The other way is to add a requirements.txt file to the package codebase.  \n", "To do so, simply add the following requirements.txt file in the same folder of the entry point (main.py):  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["package = project.packages.push(\n", "    src_path='<path to folder containing the codebase>',\n", "    package_name='face-detector',\n", "    modules=[module]\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Push The Package  \n", "Now we have all our files in one place:  \n", "- main.py  \n", "- requirements.txt  \n", "- res10_300x300_ssd_iter_140000.caffemodel  \n", "- deploy.prototxt.txt  \n", "  \n", "Time to push the package:  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["service = package.deploy(\n", "    service_name='face-detector',\n", "    init_input=[\n", "        dl.FunctionIO(name='model_filename',\n", "                      type=dl.PackageInputType.JSON,\n", "                      value='res10_300x300_ssd_iter_140000.caffemodel'),\n", "        dl.FunctionIO(name='prototxt_filename',\n", "                      type=dl.PackageInputType.JSON,\n", "                      value='deploy.prototxt.txt'),\n", "        dl.FunctionIO(name='min_confidence',\n", "                      type=dl.PackageInputType.JSON,\n", "                      value=0.5)\n", "    ],\n", "    runtime=dl.KubernetesRuntime(concurrency=1)\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Deploy A Service  \n", "The package is now ready to be deployed to the Dataloop Platform.  \n", "The runtime argument here is the service configuration. Concurrency=1 means that only one execution can run at a time (no parallel executions).  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["filters = dl.Filters(resource=dl.FiltersResource.ITEM)\n", "filters.add(field='metadata.system.mimetype', values='image*')\n", "trigger = service.triggers.create(\n", "    name='face-detector',\n", "    function_name=\"detect\",\n", "    resource=dl.TriggerResource.ITEM,\n", "    actions=dl.TriggerAction.CREATED,\n", "    filters=filters\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Model Weights As Artifacts  \n", "When working with AI, we have the package with the model code on one hand and the weight file (the results of training the model) on the other hand.  \n", "The model needs to load the weight file to be able to identify the data.  \n", "When writing FaaS (Function as a Service), we create a package with our code. Since the weight file can be very large, we do not want to push such a large file into the package; therefore, we use data objects called artifacts.  \n", "See this example how to upload model weights as artifacts.  \n", "  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}